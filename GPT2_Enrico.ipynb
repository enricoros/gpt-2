{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2 Enrico",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkgeZBZcQ5g9",
        "colab_type": "text"
      },
      "source": [
        "Download the GPT-2 tools repository, and the GPT-2 models, by using the download script which is part of the tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIgk8vuvR5o2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean\n",
        "import os\n",
        "os.chdir('/content')\n",
        "!rm -fr gpt-2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozRoeL0DNaSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the GPT-2 tools with my changes\n",
        "![ -d '/content/gpt-2' ] || git clone https://github.com/enricoros/gpt-2/ /content/gpt-2\n",
        "import os\n",
        "os.chdir('/content/gpt-2')\n",
        "# install the required libraries (assumes TF is already installed and configured)\n",
        "!pip3 install -r requirements.txt > /dev/null\n",
        "# download the pre-trained medium models\n",
        "![ -d 'models/345M' ] || python download_model.py 345M\n",
        "![ -d 'models/774M' ] || python download_model.py 774M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzfKIIgyM9K5",
        "colab_type": "text"
      },
      "source": [
        "Test for GPU to be present:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5I-_lTwM_Qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkRLWVtLTZTg",
        "colab_type": "text"
      },
      "source": [
        "We can use a cleaner output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBW-fNduTV9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reduce verbosity\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8aR4B2QOz-f",
        "colab_type": "text"
      },
      "source": [
        "Generate an **unconditional** sample, where the only hyperparameters are the top_k threshold and the temperature. Everything else is left to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhnBUdT6SYLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!time python3 src/generate_unconditional_samples.py --model_name='774M' --nsamples=1 --top_k=40 --temperature=0.7 | tee unconditional_samples.txt 2> unconditional_samples.err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnUGupojN5Hz",
        "colab_type": "text"
      },
      "source": [
        "Generate a parametric **unconditional** sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JvcLQ-3xN3tg",
        "colab": {}
      },
      "source": [
        "#@title Generate a Story; configuration:\n",
        "nsamples = 2 #@param {type:\"slider\", min:1, max:8, step:1, default:1}\n",
        "top_k = 40 #@param {type:\"slider\", min:1, max:100, step:1, default:40}\n",
        "temperature = 0.7 #@param {type:\"number\", default: 0.7}\n",
        "length = 33 #@param {type:\"slider\", min:1, max:1024, step:16, default: 1024}\n",
        "\n",
        "# run the parametric unconditional gen\n",
        "!time python3 src/generate_unconditional_samples.py --model_name='774M' --nsamples={nsamples} --batch_size={nsamples} --top_k={top_k} --temperature={temperature} --length={length} | tee unconditional_samples.txt 2> unconditional_samples.err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkmbEpJyUS7u",
        "colab_type": "text"
      },
      "source": [
        "Configure for South Park"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-fJRobEVa0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/BobAdamsEE/SouthParkData/master/by-season/Season-16.csv --quiet --show-progress\n",
        "!wget https://raw.githubusercontent.com/BobAdamsEE/SouthParkData/master/by-season/Season-17.csv --quiet --show-progress\n",
        "!wget https://raw.githubusercontent.com/BobAdamsEE/SouthParkData/master/by-season/Season-18.csv --quiet --show-progress\n",
        "!wget https://raw.githubusercontent.com/BobAdamsEE/SouthParkData/master/by-season/Season-19.csv --quiet --show-progress"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX85X6t60Kxm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1I2WTcs0LTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 -W ignore src/interactive_conditional_samples.py --model_name=345M --nsamples=2 --batch_size=2 --length=100 --top_k=40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3GfSq7zIV9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0YiIDjuBayQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}